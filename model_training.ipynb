{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import librosa\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Training and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Crema'\n",
    "train_dir = './Train'\n",
    "valid_dir = './Valid'\n",
    "\n",
    "emotion_dict = defaultdict(list)\n",
    "emotion2idx_dict = {}\n",
    "idx2emotion_dict = {}\n",
    "train_data_ratio = 0.88\n",
    "\n",
    "# collect files with same emotion into a dictionary\n",
    "for idx, filename in enumerate(os.listdir(data_dir)):\n",
    "    emotion = filename.split('_')[2]\n",
    "    emotion_dict[emotion].append(filename)\n",
    "\n",
    "# create training and validation datasets\n",
    "os.makedirs(train_dir)\n",
    "os.makedirs(valid_dir)\n",
    "\n",
    "for idx, emotion in enumerate(emotion_dict):\n",
    "    emotion2idx_dict[emotion] = idx\n",
    "    idx2emotion_dict[idx] = emotion\n",
    "    \n",
    "    random.shuffle(emotion_dict[emotion])\n",
    "\n",
    "    split_point = int(len(emotion_dict[emotion]) * train_data_ratio)\n",
    "    train_split = emotion_dict[emotion][:split_point]\n",
    "    valid_split = emotion_dict[emotion][split_point:]\n",
    "\n",
    "    for filename in train_split:\n",
    "        shutil.copy2(os.path.join(data_dir, filename), train_dir)\n",
    "    \n",
    "    for filename in valid_split:\n",
    "        shutil.copy2(os.path.join(data_dir, filename), valid_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech_Dataset(Dataset):\n",
    "    def __init__(self, data_dir, emotion2idx_dict, spectrogram_length):\n",
    "        self.data_dir = data_dir\n",
    "        self.emotion2idx_dict = emotion2idx_dict\n",
    "        self.spectrogram_length = spectrogram_length\n",
    "\n",
    "        self.filenames = os.listdir(data_dir)\n",
    "        self.labels = [self.emotion2idx_dict[filename.split('_')[2]] for filename in self.filenames]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # load the speech file and calculate spectrogram\n",
    "        speech_audio, _ = librosa.load(os.path.join(self.data_dir, filename), sr = 16000 * 0.8)\n",
    "        spectrogram = librosa.stft(speech_audio, n_fft=1024, hop_length=160, center=False, win_length=1024)\n",
    "        spectrogram = abs(spectrogram)\n",
    "        \n",
    "        feature_size, length = spectrogram.shape\n",
    "\n",
    "        # modify the length of the spectrogram to be the same as the specified length\n",
    "        if length > self.spectrogram_length:\n",
    "            spectrogram = spectrogram[:, :self.spectrogram_length]\n",
    "        else:\n",
    "            cols_needed = self.spectrogram_length - length\n",
    "            spectrogram = np.concatenate((spectrogram, np.zeros((feature_size, cols_needed))), axis=1)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Calculate the new dimensions (80% of the original size)\n",
    "        new_width = int(spectrogram.shape[1] * 0.8)\n",
    "        new_height = int(spectrogram.shape[0] * 0.8)\n",
    "\n",
    "        # Resize the array using the resize function with interpolation\n",
    "        spectrogram = np.resize(spectrogram, (new_height, new_width))\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.expand_dims(spectrogram.astype(np.float32), axis=0) , label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the longest spectrogram length in the training dataset\n",
    "spectrogram_length = 0\n",
    "feature_size = 0\n",
    "\n",
    "for filename in os.listdir(train_dir):\n",
    "    speech_audio, _ = librosa.load(os.path.join(train_dir, filename), sr = 16000 * 0.8)\n",
    "    spectrogram = librosa.stft(speech_audio, n_fft=1024, hop_length=160, center=False, win_length=1024)\n",
    "    spectrogram = abs(spectrogram)\n",
    "    feature_size, length = spectrogram.shape\n",
    "\n",
    "    if length > spectrogram_length:\n",
    "        spectrogram_length = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and validation datasets and dataloaders\n",
    "train_dataset = Speech_Dataset(train_dir, emotion2idx_dict, spectrogram_length)\n",
    "valid_dataset = Speech_Dataset(valid_dir, emotion2idx_dict, spectrogram_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Number of Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SpectrogramCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Linear layer with reduced precision (FP16)\n",
    "        self.fc1 = nn.Linear(1605632, 64).to(torch.float16)\n",
    "        \n",
    "        # Convert to FP32 before the last linear layer\n",
    "        self.fc1_to_fp32 = nn.Sequential(nn.ReLU())\n",
    "        \n",
    "        # The last linear layer, operates in FP32\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.reshape(-1)\n",
    "        x = x.to(torch.float16)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for saving the checkpoint files\n",
    "checkpoint_dir = './checkpoint'\n",
    "os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(emotion2idx_dict)\n",
    "num_epochs = 15\n",
    "\n",
    "# Remember to change the output filename and model\n",
    "output_filename = os.path.join(checkpoint_dir, 'SpectrogramCNN.pth')\n",
    "model = SpectrogramCNN(num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1605632 and 39984x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m total_samples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Desktop/ODML/train-on-jetson-nano/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/ODML/train-on-jetson-nano/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb#X34sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb#X34sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat16)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb#X34sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb#X34sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/quackyducky/Desktop/ODML/train-on-jetson-nano/model_training.ipynb#X34sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/Desktop/ODML/train-on-jetson-nano/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/ODML/train-on-jetson-nano/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ODML/train-on-jetson-nano/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1605632 and 39984x64)"
     ]
    }
   ],
   "source": [
    "# training script\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}, Training Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Training Latency in seconds: {}'.format(round(end_time - start_time, 3)))\n",
    "\n",
    "torch.save(model.state_dict(), output_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Count: 3213278\n",
      "Total FLOPs Count: 81792624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 896/896 [00:14<00:00, 60.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 44.20%\n",
      "Average Inference Latency in seconds: 0.01645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = Sample_Rate_80_Percent_SpectrogramCNN(num_classes)\n",
    "model.load_state_dict(torch.load(output_filename))\n",
    "model.eval()\n",
    "\n",
    "# calculate total number of parameters in the model \n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Parameter Count: {total_params}\")\n",
    "\n",
    "# calculate FLOPs count for a single inference\n",
    "channel = 1\n",
    "height = feature_size\n",
    "width = spectrogram_length\n",
    "kernel_size = 3\n",
    "\n",
    "FLOPs_count = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param_type = name.split('.')[1]\n",
    "\n",
    "    if name.startswith('conv'):\n",
    "        if param_type ==  'weight':\n",
    "            out_channel, in_channel, kernel_size, kernel_size = param.size()\n",
    "            mul_FLOPs = kernel_size * kernel_size * in_channel\n",
    "            add_FLOPs = (kernel_size * kernel_size - 1) * in_channel + (in_channel - 1)\n",
    "            single_kernel_FLOPs = mul_FLOPs + add_FLOPs\n",
    "            FLOPs_count += single_kernel_FLOPs * height * width * out_channel\n",
    "            channel = out_channel\n",
    "        elif param_type == 'bias':\n",
    "            FLOPs_count += channel * height * width\n",
    "    elif name.startswith('bn'):\n",
    "        FLOPs_count += channel * height * width\n",
    "        if param_type == 'bias':\n",
    "            height = height // 2\n",
    "            width = width // 2\n",
    "    elif name.startswith('fc'):\n",
    "        if param_type ==  'weight':\n",
    "            out_neuron, in_neuron = param.size()\n",
    "            FLOPs_count += (2 * in_neuron - 1) * out_neuron\n",
    "        elif param_type == 'bias':\n",
    "            FLOPs_count += param.size()[0]\n",
    "\n",
    "print(\"Total FLOPs Count: {}\".format(FLOPs_count))\n",
    "\n",
    "# validation script\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(valid_dataloader):\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "accuracy = correct_predictions / total_samples * 100\n",
    "print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "print('Average Inference Latency in seconds: {}'.format(round((end_time - start_time) / len(valid_dataloader), 6)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_Final_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
